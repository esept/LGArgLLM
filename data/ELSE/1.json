[
  [
    {
      "role": "system",
      "content": "You are given a **claim** and a corresponding argument `Ai`.\n\nYour task is to:\n1. **Verify** whether the `cate` (P or S) originally assigned to `Ai` is logically valid;\n2. Assign a **confidence score** (0.0\u20131.0) to `Ai`, based on its strength and reliability;\n3. Recursively construct an **attack tree** starting from `Ai`, introducing logically valid counter-arguments and rebuttals;\n4. For each attack, assign a **strength score** (0.0\u20131.0) indicating how effectively the attacker undermines the target;\n5. Output everything in strict **YAML** format only.\n\n---\n\n## Step 1: Verify `Ai.cate`\n- If `cate: P` \u2192 argument supports the **truth** of the claim\n- If `cate: S` \u2192 argument supports the **negation** of the claim\nIf the assigned cate is wrong, **correct it** in your output. Output only the correct value.\n\n---\n\n## Step 2: Assign `Ai.confidence`\nAssign a score between 0.0 and 1.0:\n- 0.9\u20131.0 \u2192 Widely accepted facts, textbook truths\n- 0.7\u20130.89 \u2192 General, reliable reasoning\n- 0.5\u20130.69 \u2192 Context-sensitive or partial validity\n- <0.5 \u2192 Speculative, rare-case, or weak support\n\n---\n\n## Step 3: Build the Attack Tree\n- Generate B-layer arguments that **directly attack Ai**\n- For each Bi, recursively generate counter-arguments (C-layer, etc.)\n- Each attack should include:\n  ```yaml\n  attacks:\n    TargetID: strength_score\n````\n\nwhere `strength_score` is between 0.0 and 1.0\nUse a variety of attack types:\n* Logical contradictions\n* Statistical counterexamples\n* Scope/relevance/context rebuttals\n* Challenges to definitions or authority\n\nFor rare-case attacks, **reduce both confidence and attack strength** accordingly.\n\n---\n\n## Output Format (YAML)\n\n```yaml\nArguments:\n  - id: A3\n    text: \"Exact input argument text.\"\n    cate: \"P\"\n    confidence: 0.91\n    attacks: {}\n\n  - id: B1\n    text: \"First-level counter-argument to A3.\"\n    confidence: 0.68\n    attacks:\n      A3: 0.75\n\n  - id: C1\n    text: \"Counter-argument to B1.\"\n    confidence: 0.88\n    attacks:\n      B1: 0.85\n```\n\n---\n\n### Rules:\n\n* Keep the original `Ai.id` and `Ai.text` **unchanged**\n* Output only under `Arguments:` root\n* Use **exact YAML syntax** (no comments, no JSON-like elements)\n* Use double quotes for all strings\n* `attacks` is a **dictionary**, not a list\n\n---\n\n## Summary\n\nBuild a structured, confidence-calibrated, logically grounded **YAML-format attack tree** starting from a single argument.\nVerify or correct its `cate`, assign accurate confidence, and recursively generate coherent counter-arguments with attack strengths.\n"
    },
    {
      "role": "user",
      "content": "{'claim': 'll', 'support_argument': {'role': 'system', 'content': 'You are given a **claim**.\\n---\\n### Step 1: Estimate Confidence Score\\nEstimate the truthfulness of the claim based on **mainstream, widely accepted knowledge**.\\n- Output a number between **0.0 and 1.0**:\\n  - 1.0 means **certainly true**\\n  - 0.0 means **certainly false**\\n  - Values in between reflect varying degrees of confidence\\n\\n---\\n### Step 2: Generate Arguments Based on Confidence\\nBased on the confidence score:\\n- If confidence > 0.80 \u2192 generate **4 to 8 arguments that support the claim**\\n- If confidence < 0.20 \u2192 generate **4 to 8 arguments that support the negation of the claim**\\n- If 0.20 \u2264 confidence \u2264 0.80 \u2192 generate **4 to 8 arguments**, each taking either side **randomly** (but each argument must clearly choose a side)\\n\\nEach argument must have:\\n- A distinct, logical line of reasoning\\n- A `cate` field:\\n  - `\"P\"` if the argument supports the claim\\n  - `\"S\"` if the argument supports the negation of the claim\\n\\n---\\n### Rules for All Arguments\\n- Arguments must be logically and semantically **distinct**\\n- Do **not** include:\\n  - Rare edge cases, speculative or humorous content\\n  - Vague temporal references (\"recently\", \"now\") unless tied to a known time point (assume year = 2025)\\n  - Subjective adjectives without measurable grounding (\"better\", \"worse\")\\n  - Cultural stereotypes or idioms\\n- All content must be based on **mainstream, widely accepted sources** such as scientific consensus, legal norms, textbooks, or general human knowledge\\n\\n---\\n### Output Format (YAML)\\n```yaml\\nConfidence: 0.83\\nArguments:\\n  - id: A1\\n    cate: \"P\"\\n    text: \"Example of a distinct, well-grounded argument supporting the claim.\"\\n  - id: A2\\n    cate: \"P\"\\n    text: \"Another clear, distinct argument in favor of the claim.\"\\n```\\nUse exactly 4\u20138 arguments\\nUse strict YAML syntax with double quotes for all strings\\nOutput only YAML content \u2014 no commentary or explanation'}}"
    }
  ],
  [
    {
      "role": "system",
      "content": "You are given a **claim** and a corresponding argument `Ai`.\n\nYour task is to:\n1. **Verify** whether the `cate` (P or S) originally assigned to `Ai` is logically valid;\n2. Assign a **confidence score** (0.0\u20131.0) to `Ai`, based on its strength and reliability;\n3. Recursively construct an **attack tree** starting from `Ai`, introducing logically valid counter-arguments and rebuttals;\n4. For each attack, assign a **strength score** (0.0\u20131.0) indicating how effectively the attacker undermines the target;\n5. Output everything in strict **YAML** format only.\n\n---\n\n## Step 1: Verify `Ai.cate`\n- If `cate: P` \u2192 argument supports the **truth** of the claim\n- If `cate: S` \u2192 argument supports the **negation** of the claim\nIf the assigned cate is wrong, **correct it** in your output. Output only the correct value.\n\n---\n\n## Step 2: Assign `Ai.confidence`\nAssign a score between 0.0 and 1.0:\n- 0.9\u20131.0 \u2192 Widely accepted facts, textbook truths\n- 0.7\u20130.89 \u2192 General, reliable reasoning\n- 0.5\u20130.69 \u2192 Context-sensitive or partial validity\n- <0.5 \u2192 Speculative, rare-case, or weak support\n\n---\n\n## Step 3: Build the Attack Tree\n- Generate B-layer arguments that **directly attack Ai**\n- For each Bi, recursively generate counter-arguments (C-layer, etc.)\n- Each attack should include:\n  ```yaml\n  attacks:\n    TargetID: strength_score\n````\n\nwhere `strength_score` is between 0.0 and 1.0\nUse a variety of attack types:\n* Logical contradictions\n* Statistical counterexamples\n* Scope/relevance/context rebuttals\n* Challenges to definitions or authority\n\nFor rare-case attacks, **reduce both confidence and attack strength** accordingly.\n\n---\n\n## Output Format (YAML)\n\n```yaml\nArguments:\n  - id: A3\n    text: \"Exact input argument text.\"\n    cate: \"P\"\n    confidence: 0.91\n    attacks: {}\n\n  - id: B1\n    text: \"First-level counter-argument to A3.\"\n    confidence: 0.68\n    attacks:\n      A3: 0.75\n\n  - id: C1\n    text: \"Counter-argument to B1.\"\n    confidence: 0.88\n    attacks:\n      B1: 0.85\n```\n\n---\n\n### Rules:\n\n* Keep the original `Ai.id` and `Ai.text` **unchanged**\n* Output only under `Arguments:` root\n* Use **exact YAML syntax** (no comments, no JSON-like elements)\n* Use double quotes for all strings\n* `attacks` is a **dictionary**, not a list\n\n---\n\n## Summary\n\nBuild a structured, confidence-calibrated, logically grounded **YAML-format attack tree** starting from a single argument.\nVerify or correct its `cate`, assign accurate confidence, and recursively generate coherent counter-arguments with attack strengths.\n"
    },
    {
      "role": "user",
      "content": "{'claim': 'll', 'support_argument': {'role': 'user', 'content': 'll'}}"
    }
  ]
]